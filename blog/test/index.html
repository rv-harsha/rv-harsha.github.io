<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>test</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.1/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="../../index.css"></head>
  <body>
    <div id="loading">
      <div id="spinner"></div>
    </div>
    <a href="/" class="go_back"><i class="fas fa-arrow-left"></i></a>
    <div id="background_overlay"></div>
    <div id="background" style="background: url(top_image.png) center;"></div>
    <table id="profile_blog">
      <tbody>
        <tr>
          <td style="width:8vw;"><div id="profile_img_blog"></div></td>
          <td style="width:52vw;">
            <div id="username_blog"></div>
          </td>
        </tr>
      </tbody>
    </table>
    <div id="blog-display">
      <h1 id="blog_title">test</h1>
      <h2 id="blog_sub_title">test</h2>
      <div id="blog"><img src="img_0.png" alt="blog image 0" id="img_0_original"><p id="para_0_original">


<title>hw03_MINST_MLP.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>


  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
</p><pre class="hljs"><code><div><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> h5py
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> log, floor
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm_notebook <span class="hljs-keyword">as</span> tqdm    <span class="hljs-comment"># Progress Bar</span>
<span class="hljs-keyword">from</span> prettytable <span class="hljs-keyword">import</span> PrettyTable       <span class="hljs-comment"># Tabulate results. Install it by running pip install PrettyTable</span>
</div></code></pre>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">formatting_numbers</span><span class="hljs-params">(num)</span>:</span> 
    units = [<span class="hljs-string">''</span>, <span class="hljs-string">'K'</span>, <span class="hljs-string">'M'</span>, <span class="hljs-string">'G'</span>, <span class="hljs-string">'T'</span>, <span class="hljs-string">'P'</span>]
    k = <span class="hljs-number">1000.0</span>
    value = int(floor(log(num, k)))
    <span class="hljs-keyword">return</span> <span class="hljs-string">'%.2f%s'</span> % (num / k**value, units[value])

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extract_data</span><span class="hljs-params">(fname)</span>:</span>
    <span class="hljs-string">"""
    This function reads the objects from the hdf5 file and 
    returns a dictionary object 
    Args:- fname (string): File name 
    Returns:- dictionary: object with keys and values of objects from file
    """</span>
    data = {}
    <span class="hljs-keyword">with</span> h5py.File(fname, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> hf:
        values = [np.array(val) <span class="hljs-keyword">for</span> val <span class="hljs-keyword">in</span> hf.values()]     <span class="hljs-comment"># Convert hdf5 data to numpy array</span>
        data = dict(zip(hf.keys(),values))                  <span class="hljs-comment"># Add the data to a dictionary </span>
    <span class="hljs-keyword">return</span> data
</div></code></pre>
<h2 id="1-activation-layer">1. Activation Layer</h2>
<ul>
<li>The operations performed by activation layer are defined in this class</li>
<li>Supports tanh, relu and softmax activation</li>
<li>The gradient of these activation functions applied during backward pass are also defined here</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ActivationLayer</span><span class="hljs-params">(object)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,name,activation_type)</span>:</span>
        
        self.name = name
        self.activation_type = activation_type

        supported_activations = [<span class="hljs-string">'softmax'</span>,<span class="hljs-string">'tanh'</span>,<span class="hljs-string">'relu'</span>]

        <span class="hljs-keyword">if</span> self.activation_type <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> supported_activations:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">'Activation Function {} not supported'</span>
        .format(self.activation_function))
        
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">activation_function</span><span class="hljs-params">(self,input_val)</span>:</span>

        <span class="hljs-keyword">if</span> self.activation_type==<span class="hljs-string">'relu'</span>:
            <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>,input_val)

        <span class="hljs-comment"># Clipping input to avoid 'nan's on overflow or underflow</span>
        input_val = np.clip( input_val, <span class="hljs-number">-500</span>, <span class="hljs-number">500</span> )

        <span class="hljs-keyword">if</span> self.activation_type==<span class="hljs-string">'tanh'</span>:
            <span class="hljs-keyword">return</span> (np.exp(<span class="hljs-number">2</span>*input_val) <span class="hljs-number">-1</span> ) / (np.exp(<span class="hljs-number">2</span>*input_val)+<span class="hljs-number">1</span>)
        <span class="hljs-keyword">elif</span> self.activation_type==<span class="hljs-string">'softmax'</span>:
            <span class="hljs-keyword">return</span> np.exp(input_val) / np.exp(input_val).sum(axis=<span class="hljs-number">0</span>)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gradient_activation_function</span><span class="hljs-params">(self,input_val)</span>:</span>

        output_val = np.copy(input_val)
        
        <span class="hljs-keyword">if</span> self.activation_type==<span class="hljs-string">'relu'</span>:
            <span class="hljs-comment">#Heaviside function</span>
            output_val[input_val&gt;<span class="hljs-number">0</span>] = <span class="hljs-number">1</span>
            output_val[input_val&lt;<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>
            output_val[input_val==<span class="hljs-number">0</span>] = <span class="hljs-number">0.5</span>  
            <span class="hljs-keyword">return</span> output_val
        <span class="hljs-keyword">elif</span> self.activation_type==<span class="hljs-string">'tanh'</span>:
            <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>-np.power( self.activation_function(output_val),<span class="hljs-number">2</span>)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, input)</span>:</span>
        <span class="hljs-keyword">return</span> self.activation_function(input)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self,prev_pre_activation_gradients, previous_weights)</span>:</span>
        <span class="hljs-keyword">return</span>  np.dot(previous_weights.T , prev_pre_activation_gradients)

</div></code></pre>
<h2 id="2-pre-activation-layer">2. Pre Activation Layer</h2>
<ul>
<li>Supports multiple initializations type for the weights: uniform, glorot, he_normal, he_uniform</li>
<li>Cache the gradient of the weights , biases and of its pre activation</li>
<li>Contains functions to make forward and backward pass</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PreActivationLayer</span><span class="hljs-params">(object)</span>:</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,name,dimensions,initialization_type=<span class="hljs-string">'glorot'</span>)</span>:</span>
        
        self.name = name
        self.dimensions = dimensions 
        self.weights = <span class="hljs-literal">None</span>
        self.bias = <span class="hljs-literal">None</span> 
        self.dw = <span class="hljs-literal">None</span>
        self.db = <span class="hljs-literal">None</span>
        self.d_preactivation = <span class="hljs-literal">None</span>
        self.initialize_weigths(initialization_type)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_weigths</span><span class="hljs-params">(self,initialization_type)</span>:</span>

        self.bias = np.zeros((self.dimensions[<span class="hljs-number">0</span>],<span class="hljs-number">1</span>))
        <span class="hljs-keyword">if</span> initialization_type ==<span class="hljs-string">'normal'</span>:
            self.weights = np.random.normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,self.dimensions)
        <span class="hljs-keyword">elif</span> initialization_type ==<span class="hljs-string">'glorot'</span>:
            uniform_range = np.sqrt(<span class="hljs-number">6</span>) / np.sqrt(np.sum(self.dimensions))
            self.weights = np.random.uniform(<span class="hljs-number">-1</span> * uniform_range, \
                uniform_range ,self.dimensions)
        <span class="hljs-keyword">elif</span> initialization_type ==<span class="hljs-string">'he_uniform'</span>:
            uniform_range = np.sqrt(<span class="hljs-number">6</span>) / np.sqrt(self.dimensions[<span class="hljs-number">0</span>])
            self.weights = np.random.uniform(<span class="hljs-number">-1</span> * uniform_range, \
                uniform_range ,self.dimensions)
        <span class="hljs-keyword">elif</span> initialization_type ==<span class="hljs-string">'he_normal'</span>:
            normal_range = np.sqrt(<span class="hljs-number">2</span>)/np.sqrt(self.dimensions[<span class="hljs-number">0</span>])
            self.weights = np.random.uniform(<span class="hljs-number">0</span>, normal_range ,self.dimensions)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">'Not supported Initialization type {}'</span>\
                .format(initialization_type))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self,input)</span>:</span>
        <span class="hljs-keyword">return</span> np.dot(self.weights, input) + self.bias

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, prev_activation_output, prev_weight, gradient_activation, \
        gradient_pre_activation)</span>:</span>

        n_items_batch = prev_activation_output.shape[<span class="hljs-number">1</span>]
        self.d_preactivation = np.multiply(gradient_activation, gradient_pre_activation)
        self.dw = (<span class="hljs-number">1.0</span>/n_items_batch )*  np.dot(self.d_preactivation, prev_activation_output.T)
        self.db = (<span class="hljs-number">1.0</span>/n_items_batch )* np.sum(self.d_preactivation,axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)

</div></code></pre>
<h2 id="3-neural-layer">3. Neural Layer</h2>
<ul>
<li>This class holds the object of the pre activation and the activation layer</li>
<li>Contains forward and backward functions that would call the pre activation and activation for forward and backward</li>
<li>Caches the forward pass output to be used in the backward propagation</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NeuralLayer</span><span class="hljs-params">(object)</span>:</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,name,dimension,parent,activation_type,initialization_type)</span>:</span>

        self.name = name
        self.dimension = dimension

        self.pre_activation = PreActivationLayer(name+<span class="hljs-string">'_pre_activation'</span>,\
            self.dimension,initialization_type)
        self.output_activation=ActivationLayer(name+<span class="hljs-string">'_activation'</span>,activation_type)

        self.cached_pre_activation_output_forward = <span class="hljs-literal">None</span>
        self.cached_activation_output_forward = <span class="hljs-literal">None</span> 

        self.cached_preactivation_gradient = <span class="hljs-literal">None</span>
        self.cached_activation_gradient = <span class="hljs-literal">None</span>
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self,input)</span>:</span>

        self.cached_pre_activation_output_forward = self.pre_activation.forward(input)
        self.cached_activation_output_forward =\
            self.output_activation.forward(self.cached_pre_activation_output_forward)

        <span class="hljs-keyword">return</span> self.cached_activation_output_forward

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self,prev_activation_output, prev_weight, prev_pre_activation_gradients)</span>:</span>
        
        gradient_pre_activation = self.output_activation\
        .gradient_activation_function(self.cached_pre_activation_output_forward)

        self.cached_activation_gradient = self.output_activation\
        .backward(prev_pre_activation_gradients, prev_weight)
        
        self.pre_activation\
        .backward(prev_activation_output,prev_weight
        ,self.cached_activation_gradient, gradient_pre_activation)

        self.cached_preactivation_gradient = self.pre_activation.d_preactivation

</div></code></pre>
<h1 id="neural-net">Neural Net</h1>
<ul>
<li>Wrapper class for the above class objects</li>
<li>Train and test the model by computing forward, backward pass and updating the parameters</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NeuralNet</span><span class="hljs-params">(object)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(
        self,hidden_dims=<span class="hljs-params">(<span class="hljs-number">512</span>,<span class="hljs-number">128</span>)</span>, 
        n_hidden=<span class="hljs-number">2</span>, 
        mode=<span class="hljs-string">'train'</span>,
        datapath=<span class="hljs-string">'datasets'</span>,
        model_path=None,
        seed=<span class="hljs-number">1069</span>,
        min_batch_size=<span class="hljs-number">128</span>,
        learning_rate = <span class="hljs-number">0.1</span>, 
        hidden_activations= [<span class="hljs-string">'relu'</span>,<span class="hljs-string">'relu'</span>], 
        initialization_type=<span class="hljs-string">'glorot'</span>)</span>:</span>

        <span class="hljs-string">"""
        This function recieves the parameters for this class , intitilaizes to compute 
        the weights, cost, etc.. for all the layers and  returns the losses for test, 
        train and validation set data. 

        Args:
            hidden_dims (tuple, optional): [description]. Defaults to (512,128).
            n_hidden (int, optional): [description]. Defaults to 2.
            mode ([type], optional): [description]. Defaults to 'train'\.
            datapath (str, optional): [description]. Defaults to 'datasets'.
            model_path ([type], optional): [description]. Defaults to None.
            seed (int, optional): [description]. Defaults to 1069.
            min_batch_size ([type], optional): [description]. Defaults to 128\.
            learning_rate (float, optional): [description]. Defaults to 0.1.
            hidden_activations (list, optional): [description]. Defaults to ['relu','relu'].
            initialization_type (str, optional): [description]. Defaults to 'glorot'.
        """</span>
        
        <span class="hljs-comment"># Important Hyperparameters</span>
        self.min_batch_size = min_batch_size
        self.hidden_dims = hidden_dims
        self.hidden_activations = hidden_activations
        self.learning_rate = learning_rate
        self.num_iterations = <span class="hljs-number">50</span>
        self.n_hidden = n_hidden
        self.initialization_type = initialization_type

        <span class="hljs-comment"># Other hyperparameters</span>
        self.seed = seed
        self.datapath = datapath
        self.model_path = model_path
        self.n_labels = <span class="hljs-literal">None</span>
        self.input_layer = <span class="hljs-literal">None</span>
        self.output_layer = <span class="hljs-literal">None</span>
        self.input_dim = <span class="hljs-literal">None</span>
        self.train_x = <span class="hljs-literal">None</span>
        self.train_y = <span class="hljs-literal">None</span>
        self.val_x = <span class="hljs-literal">None</span>
        self.val_y = <span class="hljs-literal">None</span>
        self.test_x = <span class="hljs-literal">None</span>
        self.test_y = <span class="hljs-literal">None</span>
        self.hidden_layers = []    

        <span class="hljs-comment"># validate the inputs and load data</span>
        self.validate_inputs()
        self.load_data()
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_data</span><span class="hljs-params">(self)</span>:</span>
        
        <span class="hljs-comment"># Get the dictionary of keys and data from hdf5 files</span>
        train_dataset = extract_data(<span class="hljs-string">'mnist_traindata.hdf5'</span>)
        test_dataset = extract_data(<span class="hljs-string">'mnist_testdata.hdf5'</span>)

        <span class="hljs-comment"># Normalize the data to scale from [0-255] to [0-1]</span>
        fac = <span class="hljs-number">1</span> / <span class="hljs-number">255</span>
        train_data = np.asfarray(train_dataset[<span class="hljs-string">"xdata"</span>]) * fac 
        test_data = np.asfarray(test_dataset[<span class="hljs-string">"xdata"</span>]) * fac 

        <span class="hljs-comment"># Intialize the feature set for test, train and validation</span>
        self.train_x = train_data[:<span class="hljs-number">-10000</span>]       <span class="hljs-comment"># Uncomment this to train all models</span>
        <span class="hljs-comment"># self.train_x = train_data               # Uncomment this to train best model</span>
        self.val_x = train_data[<span class="hljs-number">-10000</span>:]
        self.test_x = test_data

        <span class="hljs-comment"># Split labels for test, train, validate </span>
        train_y = train_dataset[<span class="hljs-string">"ydata"</span>][:<span class="hljs-number">-10000</span>]       <span class="hljs-comment"># Uncomment this to train all models</span>
        <span class="hljs-comment"># train_y = train_dataset["ydata"]          # Uncomment this to train best model</span>
        self.train_y =  np.argmax(train_y.astype(int),axis=<span class="hljs-number">1</span>)
        
        val_y = train_dataset[<span class="hljs-string">"ydata"</span>][<span class="hljs-number">-10000</span>:]
        self.val_y =  np.argmax(val_y.astype(int),axis=<span class="hljs-number">1</span>)

        test_y = test_dataset[<span class="hljs-string">"ydata"</span>]
        self.test_y =  np.argmax(test_y.astype(int),axis=<span class="hljs-number">1</span>)

        self.input_dim = self.train_x.shape[<span class="hljs-number">1</span>]
        self.n_labels = np.unique(self.train_y).shape[<span class="hljs-number">0</span>]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validate_inputs</span><span class="hljs-params">(self)</span>:</span>
        initialization_types = [<span class="hljs-string">'normal'</span>, <span class="hljs-string">'glorot'</span>,<span class="hljs-string">'he_uniform'</span>,<span class="hljs-string">'he_normal'</span>]

        <span class="hljs-comment"># Validate number of hidden units to number of items in dimensions array</span>
        <span class="hljs-keyword">if</span> len(self.hidden_dims) != self.n_hidden:
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">'Number of hidden unit {}\
             not equal number of items {} in hidden_dims'</span>\
             .format(len(self.hidden_dims),self.n_hidden))

        <span class="hljs-comment"># Validate initialization_types</span>
        <span class="hljs-keyword">if</span> self.initialization_type <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> initialization_types :
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">'Intiialization type {} not supported'</span>\
                .format(self.initialization_type))

        <span class="hljs-comment"># Validate if num of hidden units to number of items in hidden activations</span>
        <span class="hljs-keyword">if</span> len(self.hidden_dims) != len(self.hidden_activations):
            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">'Number of hidden unit {}\
             not equal number of items {} in hidden_activations'</span>\
             .format(len(self.hidden_dims),self.hidden_activations))
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize_weights</span><span class="hljs-params">(self,verbose=<span class="hljs-number">1</span>)</span>:</span>
        input_size = self.input_dim
        parent = <span class="hljs-literal">None</span>
        self.hidden_layers = []

        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.n_hidden):
            hidden_layer = NeuralLayer(<span class="hljs-string">'h_'</span>+str(i),(self.hidden_dims[i],input_size),\
            parent, self.hidden_activations[i], self.initialization_type)
            self.hidden_layers.append(hidden_layer)
            input_size  = self.hidden_dims[i]

        self.output_layer = NeuralLayer(<span class="hljs-string">'output'</span>,(self.n_labels,self.hidden_dims[<span class="hljs-number">-1</span>]),\
        self.hidden_layers[<span class="hljs-number">-1</span>],<span class="hljs-string">'softmax'</span>, self.initialization_type)

        <span class="hljs-keyword">if</span> verbose&gt;<span class="hljs-number">0</span>:
          self.model_summary()
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model_summary</span><span class="hljs-params">(self)</span>:</span>
        model_summary_table = PrettyTable([<span class="hljs-string">'Layer'</span>, <span class="hljs-string">'Input Shape'</span>, <span class="hljs-string">'Param #'</span>])
        model_summary_table.add_row([<span class="hljs-string">'Input'</span>, <span class="hljs-string">'(None, '</span>+str(self.input_dim)+<span class="hljs-string">')'</span>, <span class="hljs-number">0</span>])
        total_n_params = <span class="hljs-number">0</span>

        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.hidden_layers + [self.output_layer]:
            n_params = (layer.dimension[<span class="hljs-number">0</span>] * layer.dimension[<span class="hljs-number">1</span>])  + \
                layer.dimension[<span class="hljs-number">0</span>]
            input_dimensions = <span class="hljs-string">'('</span>+<span class="hljs-string">','</span>.join([str(dim) <span class="hljs-keyword">for</span> dim <span class="hljs-keyword">in</span> \
                reversed(layer.dimension)])+<span class="hljs-string">')'</span>
            model_summary_table.add_row([layer.name, input_dimensions, \
                formatting_numbers(n_params)])
            total_n_params += n_params

        model_summary_table.add_row([<span class="hljs-string">''</span>, <span class="hljs-string">'Total # Params'</span>, formatting_numbers(total_n_params)])
        print(model_summary_table)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self,input)</span>:</span>
        cached_input_val = np.transpose(input)
        <span class="hljs-keyword">for</span> hidden_indx <span class="hljs-keyword">in</span> range(self.n_hidden):
            cached_input_val = self.hidden_layers[hidden_indx]\
                .forward(cached_input_val)
        output = self.output_layer.forward(cached_input_val)
        <span class="hljs-keyword">return</span> output

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loss</span><span class="hljs-params">(self,true_labels , prediction)</span>:</span>
        eps = <span class="hljs-number">1e-15</span>  <span class="hljs-comment"># eps used to avoid nans in case of 0 as log(0) is nan</span>
        prediction = np.clip(prediction, eps, <span class="hljs-number">1</span> - eps)
        cost = <span class="hljs-number">-1</span> * (np.eye(self.n_labels)[true_labels]* np.log(prediction)).sum(axis=<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> np.average(cost)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">accuracy</span><span class="hljs-params">(self,true_labels, prediction)</span>:</span>
        prediction = np.argmax(prediction,axis=<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> np.mean(prediction == true_labels) * <span class="hljs-number">100</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax</span><span class="hljs-params">(self,input)</span>:</span>
        activation =  ActivationLayer(<span class="hljs-string">'softmax'</span>,<span class="hljs-string">'softmax'</span>)
        <span class="hljs-keyword">return</span> activation.activation_function(input)
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self,train_x,train_y)</span>:</span>
        
        n_items_batch = len(train_y)

        <span class="hljs-comment"># derivative of the loss function with respect to f(x)</span>
        self.output_layer.cached_activation_gradient = \
            <span class="hljs-number">-1</span> * (np.eye(self.n_labels)[train_y].T / self.output_layer.cached_activation_output_forward)

        <span class="hljs-comment"># derivative of the loss function with respect to a(x) of the output function</span>
        self.output_layer.cached_preactivation_gradient = \
            <span class="hljs-number">-1</span> * (np.eye(self.n_labels)[train_y].T - self.output_layer.cached_activation_output_forward)

        <span class="hljs-comment"># updating gradient of weights entering the output layer</span>
        self.output_layer.pre_activation.dw = (<span class="hljs-number">1.0</span>/n_items_batch ) *  \
            np.dot(self.output_layer.cached_preactivation_gradient,   \
                self.hidden_layers[<span class="hljs-number">-1</span>].cached_activation_output_forward.T)

        <span class="hljs-comment"># updating gradient of biases entering the output layer</span>
        self.output_layer.pre_activation.db = (<span class="hljs-number">1.0</span>/n_items_batch ) *  \
            np.sum(self.output_layer.cached_preactivation_gradient,axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)

        <span class="hljs-comment"># gradient of the preactivation of the next layer</span>
        parent_pre_activation_gradients = self.output_layer.cached_preactivation_gradient

        <span class="hljs-comment"># weights of the next preactivation layer</span>
        next_layer_weights = self.output_layer.pre_activation.weights
        <span class="hljs-keyword">for</span> layer_indx <span class="hljs-keyword">in</span> range(self.n_hidden <span class="hljs-number">-1</span>,<span class="hljs-number">-1</span>,<span class="hljs-number">-1</span>):
            <span class="hljs-keyword">if</span> layer_indx !=<span class="hljs-number">0</span>:
                prev_activation_forward = self.hidden_layers[layer_indx<span class="hljs-number">-1</span>].cached_activation_output_forward
            <span class="hljs-keyword">else</span>:
                prev_activation_forward = train_x.T
            
            <span class="hljs-comment">#applying back propagation on the hidden layer</span>
            self.hidden_layers[layer_indx].backward(\
            prev_activation_forward,\
            next_layer_weights,\
            parent_pre_activation_gradients) 

            <span class="hljs-comment"># setting the parent pre activation gradient for the next layer in the backpropagation</span>
            parent_pre_activation_gradients = self.hidden_layers[layer_indx].cached_preactivation_gradient

            <span class="hljs-comment"># setting the weights to be used by the next layer's backpropagation</span>
            next_layer_weights = self.hidden_layers[layer_indx].pre_activation.weights

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-comment">#based on the object holding the gradients, update the parameters</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,self.n_hidden):
            self.hidden_layers[i].pre_activation.weights -=\
                self.learning_rate * self.hidden_layers[i].pre_activation.dw
            self.hidden_layers[i].pre_activation.bias -=\
                self.learning_rate * self.hidden_layers[i].pre_activation.db
        <span class="hljs-comment">#updating output parameters</span>
        self.output_layer.pre_activation.weights -=\
            self.learning_rate * self.output_layer.pre_activation.dw
        self.output_layer.pre_activation.bias -=\
            self.learning_rate * self.output_layer.pre_activation.db

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(self,verbose=<span class="hljs-number">1</span>)</span>:</span>

        <span class="hljs-comment"># adding seed to avoid any changes in multiple runs due to different initialization</span>
        np.random.seed(self.seed)
        self.initialize_weights(verbose)

        train_losses = []
        val_losses = []

        <span class="hljs-keyword">if</span> verbose&gt;<span class="hljs-number">0</span>:
          progress_bar =  tqdm(total=self.num_iterations)

        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(self.num_iterations):

            <span class="hljs-comment"># Dividing the learning rate by 2 (Learning rate decay)</span>
            <span class="hljs-keyword">if</span> epoch <span class="hljs-keyword">in</span> [<span class="hljs-number">25</span>,<span class="hljs-number">40</span>]:
                self.learning_rate = self.learning_rate / <span class="hljs-number">2</span>

            <span class="hljs-comment"># Execute for every mini batch</span>
            <span class="hljs-keyword">for</span> batch_indx <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,self.train_x.shape[<span class="hljs-number">0</span>],self.min_batch_size):
                lower_range = batch_indx
                upper_range = batch_indx + self.min_batch_size
                <span class="hljs-keyword">if</span> upper_range &gt;self.train_x.shape[<span class="hljs-number">0</span>]:
                    upper_range = self.train_x.shape[<span class="hljs-number">0</span>]
                train_x = self.train_x[lower_range: upper_range,:]
                train_y = self.train_y[lower_range : upper_range]

                <span class="hljs-comment"># Compute the forward pass</span>
                self.forward(train_x)

                <span class="hljs-comment"># Compute gradients through backward propagation</span>
                self.backward(train_x,train_y)

                <span class="hljs-comment"># Update parameters based on gradients</span>
                self.update()

            <span class="hljs-comment"># Compute training loss for each epoch</span>
            train_predicted_probs = self.forward(self.train_x)
            train_loss = self.loss(self.train_y,train_predicted_probs.T)
            train_losses.append(train_loss)
            
            <span class="hljs-comment"># Computing validation loss for each epoch</span>
            val_predicted_probs = self.forward(self.val_x)
            val_loss = self.loss(self.val_y,val_predicted_probs.T)
            val_losses.append(val_loss)

            <span class="hljs-comment"># Display train and validaton loss for each epoch</span>
            <span class="hljs-keyword">if</span> verbose&gt;<span class="hljs-number">0</span>:
              progress_bar.set_postfix({ <span class="hljs-string">'Train_loss'</span>:str(np.round(train_loss,<span class="hljs-number">4</span>)), \
                  <span class="hljs-string">'val_loss'</span>:str(np.round(val_loss,<span class="hljs-number">4</span>))})

            <span class="hljs-comment"># Shuffle the data every epoch</span>
            train_indices = np.random.permutation(self.train_x.shape[<span class="hljs-number">0</span>])
            self.train_x = self.train_x[train_indices,:]
            self.train_y = self.train_y[train_indices]
            
            <span class="hljs-comment"># Showing a step in progress bar</span>
            <span class="hljs-keyword">if</span> verbose&gt;<span class="hljs-number">0</span>:
              progress_bar.update()

        <span class="hljs-comment"># Compute validation accuracy on last epoch</span>
        val_predicted_probs = self.forward(self.val_x)
        val_acc = self.accuracy(self.val_y,val_predicted_probs.T) 
        
        <span class="hljs-comment"># Compute final test accuracy on last epoch</span>
        test_predicted_probs = self.forward(self.test_x)
        test_acc = self.accuracy(self.test_y,test_predicted_probs.T)
        
        <span class="hljs-comment"># Plotting validation and training history</span>
        epoch_count = range(<span class="hljs-number">1</span>, self.num_iterations + <span class="hljs-number">1</span>)
        <span class="hljs-keyword">if</span> verbose&gt;<span class="hljs-number">0</span>:
          plt.plot(epoch_count, train_losses, <span class="hljs-string">'r--'</span>)
          plt.plot(epoch_count, val_losses, <span class="hljs-string">'b-'</span>)
          plt.axvline(x=<span class="hljs-number">25</span>,linewidth = <span class="hljs-number">1</span>, linestyle =<span class="hljs-string">"-."</span>, color =<span class="hljs-string">'green'</span>)
          plt.axvline(x=<span class="hljs-number">40</span>,linewidth = <span class="hljs-number">1</span>, linestyle =<span class="hljs-string">"-."</span>, color =<span class="hljs-string">'orange'</span>)
          plt.legend([<span class="hljs-string">'Training Loss'</span>, <span class="hljs-string">'Validation Loss'</span>])
          plt.xlabel(<span class="hljs-string">'Epoch'</span>)
          plt.ylabel(<span class="hljs-string">'Loss'</span>)
          plt.show()

        <span class="hljs-keyword">if</span> self.model_path:
            pickle.dump(self,open(self.model_path,<span class="hljs-string">'wb'</span>))
        <span class="hljs-keyword">return</span> (val_acc, val_loss, test_acc, test_predicted_probs.T)
</div></code></pre>
<h1 id="training">Training</h1>
<h3 id="weights-initialization-using-glorots-equation">Weights Initialization using Glorot's equation</h3>
<ul>
<li>Intializing weights according to equation proposed by Glorot et al 2010 http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</li>
<li>This would result in small and effective weights that would avoid problems like exploding gradients from the start or zero gradients</li>
<li>Aviods excessive saturation of activation functions</li>
<li>The default weights Initialization Type has been set to glorot's Initialization</li>
</ul>
<h1 id="report">Report</h1>
<ul>
<li>Learning Rate Decay : Initial learning rate is divided by 2 twice during training. This is done after 25th and 40th Epoch.</li>
<li>The Test Accuracy has been computed after all the training for all epochs.</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th style="text-align:center">Activation Type</th>
<th style="text-align:center">Hidden Layers</th>
<th style="text-align:center">Dimensions</th>
<th style="text-align:center">Batch Size</th>
<th style="text-align:center">Learning Rate</th>
<th style="text-align:center">Val Accuracy</th>
<th style="text-align:center">Test Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td style="text-align:center">ReLu</td>
<td style="text-align:center">2</td>
<td style="text-align:center">(512,64)</td>
<td style="text-align:center">50</td>
<td style="text-align:center">0.003</td>
<td style="text-align:center">96.289</td>
<td style="text-align:center">95.83</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:center">ReLu</td>
<td style="text-align:center">3</td>
<td style="text-align:center">(500,250,60)</td>
<td style="text-align:center">80</td>
<td style="text-align:center">0.003</td>
<td style="text-align:center">96.270</td>
<td style="text-align:center">95.99</td>
</tr>
<tr>
<td>3</td>
<td style="text-align:center">ReLu</td>
<td style="text-align:center">2</td>
<td style="text-align:center">(256,32)</td>
<td style="text-align:center">50</td>
<td style="text-align:center">0.004</td>
<td style="text-align:center">96.44</td>
<td style="text-align:center">96.2</td>
</tr>
<tr>
<td>4</td>
<td style="text-align:center">Tanh</td>
<td style="text-align:center">3</td>
<td style="text-align:center">(512,64).</td>
<td style="text-align:center">80</td>
<td style="text-align:center">0.007</td>
<td style="text-align:center">95.89</td>
<td style="text-align:center">95.30</td>
</tr>
<tr>
<td>5</td>
<td style="text-align:center">Tanh</td>
<td style="text-align:center">2</td>
<td style="text-align:center">(256,32)</td>
<td style="text-align:center">125</td>
<td style="text-align:center">0.015</td>
<td style="text-align:center">95.94</td>
<td style="text-align:center">95.61</td>
</tr>
<tr>
<td>6</td>
<td style="text-align:center">Tanh</td>
<td style="text-align:center">3</td>
<td style="text-align:center">(500,200,50)</td>
<td style="text-align:center">125</td>
<td style="text-align:center">0.009</td>
<td style="text-align:center">95.99</td>
<td style="text-align:center">95.679</td>
</tr>
</tbody>
</table>
<h2 id="analysis">Analysis</h2>
<ul>
<li>Based on the analysis from the learning curve, we can see all the models give a good fit for the above configurations</li>
<li>The model with highest Validation Accuracy is ReLu activation with 2 hidden layers with dimesions - (256,32)</li>
</ul>
<pre class="hljs"><code><div>model = NeuralNet(hidden_activations= [<span class="hljs-string">'relu'</span>,<span class="hljs-string">'relu'</span>],learning_rate=<span class="hljs-number">0.003</span>,min_batch_size=<span class="hljs-number">50</span>,hidden_dims=(<span class="hljs-number">512</span>,<span class="hljs-number">64</span>))
val_acc, val_loss, test_acc, predicted_test = model.train() 
</div></code></pre>
<pre><code>+--------+----------------+---------+
| Layer  |  Input Shape   | Param # |
+--------+----------------+---------+
| Input  |  (None, 784)   |    0    |
|  h_0   |   (784,512)    | 401.92K |
|  h_1   |    (512,64)    |  32.83K |
| output |    (64,10)     |  650.00 |
|        | Total # Params | 435.40K |
+--------+----------------+---------+



HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))
</code></pre>
<p><img src="hw03_MINST_MLP_files/hw03_MINST_MLP_11_2.svg" alt="svg"></p>
<pre class="hljs"><code><div>print(<span class="hljs-string">'Validation accuracy {}'</span>.format(val_acc))
print(<span class="hljs-string">'Test accuracy {}'</span>.format(test_acc))
</div></code></pre>
<pre><code>Validation accuracy 96.28999999999999
Test accuracy 95.83
</code></pre>
<pre class="hljs"><code><div>model = NeuralNet(hidden_activations= [<span class="hljs-string">'relu'</span>,<span class="hljs-string">'relu'</span>,<span class="hljs-string">'relu'</span>],learning_rate=<span class="hljs-number">0.003</span>,min_batch_size=<span class="hljs-number">80</span>,hidden_dims=(<span class="hljs-number">500</span>,<span class="hljs-number">250</span>,<span class="hljs-number">60</span>),n_hidden=<span class="hljs-number">3</span>)
val_acc, val_loss, test_acc, predicted_test = model.train() 
</div></code></pre>
<pre><code>+--------+----------------+---------+
| Layer  |  Input Shape   | Param # |
+--------+----------------+---------+
| Input  |  (None, 784)   |    0    |
|  h_0   |   (784,500)    | 392.50K |
|  h_1   |   (500,250)    | 125.25K |
|  h_2   |    (250,60)    |  15.06K |
| output |    (60,10)     |  610.00 |
|        | Total # Params | 533.42K |
+--------+----------------+---------+



HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))
</code></pre>
<p><img src="hw03_MINST_MLP_files/hw03_MINST_MLP_13_2.svg" alt="svg"></p>
<pre class="hljs"><code><div>print(<span class="hljs-string">'Validation accuracy {}'</span>.format(val_acc))
print(<span class="hljs-string">'Test accuracy {}'</span>.format(test_acc))
</div></code></pre>
<pre><code>Validation accuracy 96.28999999999999
Test accuracy 95.99
</code></pre>
<pre class="hljs"><code><div>model = NeuralNet(hidden_activations= [<span class="hljs-string">'relu'</span>,<span class="hljs-string">'relu'</span>],learning_rate=<span class="hljs-number">0.004</span>,min_batch_size=<span class="hljs-number">50</span>,hidden_dims=(<span class="hljs-number">256</span>,<span class="hljs-number">32</span>),n_hidden=<span class="hljs-number">2</span>)
val_acc, val_loss, test_acc, predicted_test = model.train() 
</div></code></pre>
<pre><code>+--------+----------------+---------+
| Layer  |  Input Shape   | Param # |
+--------+----------------+---------+
| Input  |  (None, 784)   |    0    |
|  h_0   |   (784,256)    | 200.96K |
|  h_1   |    (256,32)    |  8.22K  |
| output |    (32,10)     |  330.00 |
|        | Total # Params | 209.51K |
+--------+----------------+---------+



HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))
</code></pre>
<p><img src="hw03_MINST_MLP_files/hw03_MINST_MLP_15_2.svg" alt="svg"></p>
<pre class="hljs"><code><div>print(<span class="hljs-string">'Validation accuracy {}'</span>.format(val_acc))
print(<span class="hljs-string">'Test accuracy {}'</span>.format(test_acc))
</div></code></pre>
<pre><code>Validation accuracy 96.44
Test accuracy 96.2
</code></pre>
<pre class="hljs"><code><div>model = NeuralNet(hidden_activations= [<span class="hljs-string">'tanh'</span>,<span class="hljs-string">'tanh'</span>],learning_rate=<span class="hljs-number">0.007</span>,min_batch_size=<span class="hljs-number">80</span>,hidden_dims=(<span class="hljs-number">512</span>,<span class="hljs-number">64</span>))
val_acc, val_loss, test_acc, predicted_test = model.train() 
</div></code></pre>
<pre><code>+--------+----------------+---------+
| Layer  |  Input Shape   | Param # |
+--------+----------------+---------+
| Input  |  (None, 784)   |    0    |
|  h_0   |   (784,512)    | 401.92K |
|  h_1   |    (512,64)    |  32.83K |
| output |    (64,10)     |  650.00 |
|        | Total # Params | 435.40K |
+--------+----------------+---------+



HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))
</code></pre>
<p><img src="hw03_MINST_MLP_files/hw03_MINST_MLP_17_2.svg" alt="svg"></p>
<pre class="hljs"><code><div>print(<span class="hljs-string">'Validation accuracy {}'</span>.format(val_acc))
print(<span class="hljs-string">'Test accuracy {}'</span>.format(test_acc))
</div></code></pre>
<pre><code>Validation accuracy 95.89
Test accuracy 95.30999999999999
</code></pre>
<pre class="hljs"><code><div>model = NeuralNet(hidden_activations= [<span class="hljs-string">'tanh'</span>,<span class="hljs-string">'tanh'</span>],learning_rate=<span class="hljs-number">0.015</span>,min_batch_size=<span class="hljs-number">125</span>,hidden_dims=(<span class="hljs-number">256</span>,<span class="hljs-number">32</span>),n_hidden=<span class="hljs-number">2</span>)
val_acc, val_loss, test_acc, predicted_test = model.train() 
</div></code></pre>
<pre><code>+--------+----------------+---------+
| Layer  |  Input Shape   | Param # |
+--------+----------------+---------+
| Input  |  (None, 784)   |    0    |
|  h_0   |   (784,256)    | 200.96K |
|  h_1   |    (256,32)    |  8.22K  |
| output |    (32,10)     |  330.00 |
|        | Total # Params | 209.51K |
+--------+----------------+---------+



HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))
</code></pre>
<p><img src="hw03_MINST_MLP_files/hw03_MINST_MLP_19_2.svg" alt="svg"></p>
<pre class="hljs"><code><div>print(<span class="hljs-string">'Validation accuracy {}'</span>.format(val_acc))
print(<span class="hljs-string">'Test accuracy {}'</span>.format(test_acc))
</div></code></pre>
<pre><code>Validation accuracy 95.94
Test accuracy 95.61
</code></pre>
<pre class="hljs"><code><div>model = NeuralNet(hidden_activations= [<span class="hljs-string">'tanh'</span>,<span class="hljs-string">'tanh'</span>,<span class="hljs-string">'tanh'</span>],learning_rate=<span class="hljs-number">0.009</span>,min_batch_size=<span class="hljs-number">125</span>,hidden_dims=(<span class="hljs-number">500</span>,<span class="hljs-number">200</span>,<span class="hljs-number">50</span>),n_hidden=<span class="hljs-number">3</span>)
val_acc, val_loss, test_acc, predicted_test = model.train() 
</div></code></pre>
<pre><code>+--------+----------------+---------+
| Layer  |  Input Shape   | Param # |
+--------+----------------+---------+
| Input  |  (None, 784)   |    0    |
|  h_0   |   (784,500)    | 392.50K |
|  h_1   |   (500,200)    | 100.20K |
|  h_2   |    (200,50)    |  10.05K |
| output |    (50,10)     |  510.00 |
|        | Total # Params | 503.26K |
+--------+----------------+---------+



HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))
</code></pre>
<p><img src="hw03_MINST_MLP_files/hw03_MINST_MLP_21_2.svg" alt="svg"></p>
<pre class="hljs"><code><div>print(<span class="hljs-string">'Validation accuracy {}'</span>.format(val_acc))
print(<span class="hljs-string">'Test accuracy {}'</span>.format(test_acc))
</div></code></pre>
<pre><code>Validation accuracy 95.99
Test accuracy 95.67999999999999
</code></pre>
<h1 id="best-mlp-model">Best MLP Model</h1>
<pre class="hljs"><code><div><span class="hljs-comment"># Note : This model is trained for all 60,000 images = training + validation.</span>
model = NeuralNet(hidden_activations= [<span class="hljs-string">'relu'</span>,<span class="hljs-string">'relu'</span>],learning_rate=<span class="hljs-number">0.004</span>,min_batch_size=<span class="hljs-number">50</span>,hidden_dims=(<span class="hljs-number">256</span>,<span class="hljs-number">32</span>),n_hidden=<span class="hljs-number">2</span>)
val_acc, val_loss, test_acc, predicted_test = model.train() 
</div></code></pre>
<pre><code>+--------+----------------+---------+
| Layer  |  Input Shape   | Param # |
+--------+----------------+---------+
| Input  |  (None, 784)   |    0    |
|  h_0   |   (784,256)    | 200.96K |
|  h_1   |    (256,32)    |  8.22K  |
| output |    (32,10)     |  330.00 |
|        | Total # Params | 209.51K |
+--------+----------------+---------+



HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))
</code></pre>
<p><img src="hw03_MINST_MLP_files/hw03_MINST_MLP_24_2.svg" alt="svg"></p>
<pre class="hljs"><code><div>print(<span class="hljs-string">'Best Model Test accuracy {}'</span>.format(test_acc))
</div></code></pre>
<pre><code>Best Model Test accuracy 96.56
</code></pre>



<p></p></div>
    </div>
    <div id="footer_blog">
      <a href="https://github.com/imfunniee" target="_blank">made on earth by a human</a>
    </div>
    <script type="text/javascript">
      setTimeout(function() {
        document.getElementById("loading").classList.add("animated");
        document.getElementById("loading").classList.add("fadeOut");
        setTimeout(function() {
          document.getElementById("loading").classList.remove("animated");
          document.getElementById("loading").classList.remove("fadeOut");
          document.getElementById("loading").style.display = "none";
        }, 800);
      }, 1500);
      $.getJSON("../../config.json", function(user) {
        var icon = document.createElement("link");
        icon.setAttribute("rel", "icon");
        icon.setAttribute("href", user[0].userimg);
        icon.setAttribute("type", "image/png");
        document.getElementsByTagName("head")[0].appendChild(icon);
        document.getElementById(
          "profile_img_blog"
        ).style.background = `url('${user[0].userimg}') center center`;
        document.getElementById(
          "username_blog"
        ).innerHTML = `<span style="display:${
          user[0].name == null || !user[0].name ? "none" : "block"
        };">${user[0].name}</span>@${user[0].username}<b id="blog_time"></b>`;

        if ((user[0].theme = "dark.css")) {
          document.querySelector("#background_overlay").style.background =
            "linear-gradient(0deg, rgba(10, 10, 10, 1), rgba(10, 10, 10, 0.1))";
        } else {
          document.querySelector("#background_overlay").style.background =
            "linear-gradient(0deg, rgba(255, 255, 255, 1), rgba(255, 255, 255, 0.1))";
        }
      });
    </script>
  

</body></html>